{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "ds = ray.data.read_json('../etl_out/partition_by_date')\n",
    "#ds = ds.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModel\n",
    "from llama_index.core.node_parser import  SentenceSplitter #SimpleNodeParser\n",
    "from llama_index.core.schema import Node\n",
    "from llama_index.core.schema import Document\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import copy\n",
    "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "\n",
    "# Suppress FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def convert_documents_into_nodes(data: Dict[str, np.ndarray]) -> List[Dict[str, Node]]:\n",
    "    splitter = SentenceSplitter()\n",
    "    all_text = data['title'] + data['content']\n",
    "    doc = Document(text = all_text )\n",
    "    del data[\"title\"]\n",
    "    del data[\"content\"]\n",
    "    doc.metadata = data\n",
    "    documents = [ doc]\n",
    "    nodes = splitter.get_nodes_from_documents(documents=documents)\n",
    "    return [{\"node\": node} for node in nodes]\n",
    "\n",
    "class EmbedNodes:\n",
    "    def __init__(self):\n",
    "        import os\n",
    "        #os.environ['HTTPS_PROXY'] = 'http://100.109.83.118:808/'\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name='/data/models/bge-large-zh')\n",
    "\n",
    "    #列存储的方式\n",
    "    def __call__(self, summary_batch: Dict[str, np.ndarray]) -> Dict[str, List[Node]]:\n",
    "        nodes = []\n",
    "        summaries = []\n",
    "        all_text = (summary_batch['title']+summary_batch['content'])\n",
    "        for text, title in zip(all_text, summary_batch['title']):\n",
    "    \n",
    "            node = TextNode(text=text, id_=title)\n",
    "            nodes.append(node)\n",
    "            summaries.append(text)\n",
    "        \n",
    "        embeddings = self.embedding_model.embed_documents(summaries)\n",
    "        assert len(nodes) == len(embeddings)\n",
    "\n",
    "        for node, embedding in zip(nodes, embeddings):\n",
    "            node.embedding = embedding\n",
    "        return {\"embedded_nodes\": nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = ds.map_batches(EmbedNodes, concurrency=4, num_gpus=1, batch_size=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "Settings.llm = Ollama(model=\"qwen:7b\", request_timeout=120.0)\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"/data/models/bge-large-zh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_docs_nodes = []\n",
    "title_dict = {}\n",
    "for row in embeds.iter_rows():\n",
    "    node:Node = row[\"embedded_nodes\"]\n",
    "    if not node.id_ in title_dict:\n",
    "        title_dict[node.id_] = True\n",
    "        assert node.embedding is not None\n",
    "        stock_docs_nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stock_docs_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from chromadb.errors import DuplicateIDError\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "Settings.llm = Ollama(model=\"qwen:7b\", request_timeout=120.0)\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"/data/models/bge-large-zh\"\n",
    ")\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./stock_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"stock_summary\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store\n",
    ")\n",
    "# create your index\n",
    "# try:\n",
    "#     index = VectorStoreIndex(\n",
    "#         nodes=stock_docs_nodes, storage_context=storage_context\n",
    "#     )\n",
    "# except DuplicateIDError:\n",
    "#     print(\"duplicat id, ignore it!\")\n",
    "\n",
    "# # create a query engine and query\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What is the meaning of life?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k = 10\n",
    ")\n",
    "response = query_engine.query(\"比亚迪汽车的销售情况\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"2024年药明生物和三星生物做对比\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "ray_docs_index = VectorStoreIndex(nodes=stock_docs_nodes)\n",
    "ray_docs_index.storage_context.persist(persist_dir=\"./tmp/index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = ray_docs_index.as_query_engine()\n",
    "response = query_engine.query(\"汽车行业的领先地位的公司的具体信息\")\n",
    "#display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
